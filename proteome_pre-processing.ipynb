{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27514cb6-bb55-4ef3-a562-95b985afc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893756a-10c5-437e-a262-7e9862e74ed9",
   "metadata": {},
   "source": [
    "# KNN inputation and outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de8096f-0a59-48ff-9518-e9874cb34057",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"Li_Oi-DAP_DAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fdf0b02-516a-4cb7-ac53-fb15fed512c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, MSE=0.47941998758294685\n",
      "k=2, MSE=0.44891566604744326\n",
      "k=3, MSE=0.5123694903487279\n",
      "The best k value is 2 with MSE=0.44891566604744326\n",
      "KNN inputation finished with K = 2\n",
      "Outliers saved to 'outliers_based_on_cv.xlsx'\n"
     ]
    }
   ],
   "source": [
    "################################################################ KNN cross validaiton\n",
    "\n",
    "# Load the proteome dataset\n",
    "file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_raw.xlsx' \n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Separate the sample names, class labels, and features\n",
    "sample_names = data.iloc[:, 0]\n",
    "class_labels = data.iloc[:, 1]\n",
    "features = data.iloc[:, 2:]\n",
    "\n",
    "# Define a range of k values to test\n",
    "k_values = range(1, 4)\n",
    "\n",
    "# Create a function to evaluate the KNN imputer with cross-validation\n",
    "def evaluate_knn_imputer(k, features):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mse_values = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(features):\n",
    "        train_data, test_data = features.iloc[train_index], features.iloc[test_index]\n",
    "        \n",
    "        # Introduce missing values in the test set to simulate imputation\n",
    "        test_data_missing = test_data.copy()\n",
    "        mask = np.random.choice([True, False], size=test_data.shape, p=[0.1, 0.9])\n",
    "        test_data_missing[mask] = np.nan\n",
    "        \n",
    "        # Perform KNN imputation\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed_train_data = imputer.fit_transform(train_data)\n",
    "        imputed_test_data = imputer.transform(test_data_missing)\n",
    "        \n",
    "        # Calculate MSE only on the artificially removed values\n",
    "        true_values = test_data.values[mask]\n",
    "        imputed_values = imputed_test_data[mask]\n",
    "        \n",
    "        # Remove any NaNs before calculating MSE\n",
    "        valid_mask = ~np.isnan(true_values)\n",
    "        true_values = true_values[valid_mask]\n",
    "        imputed_values = imputed_values[valid_mask]\n",
    "        \n",
    "        mse = mean_squared_error(true_values, imputed_values)\n",
    "        mse_values.append(mse)\n",
    "    \n",
    "    return np.mean(mse_values)\n",
    "\n",
    "# Evaluate KNN imputer for each k value\n",
    "mse_scores = []\n",
    "for k in k_values:\n",
    "    mse = evaluate_knn_imputer(k, features)\n",
    "    mse_scores.append(mse)\n",
    "    print(f'k={k}, MSE={mse}')\n",
    "\n",
    "# Find the best k value\n",
    "best_k = k_values[np.argmin(mse_scores)]\n",
    "print(f'The best k value is {best_k} with MSE={min(mse_scores)}')\n",
    "\n",
    "\n",
    "################################################################ KNN inputation \n",
    "# Script validÃ© \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Load the proteome dataset\n",
    "file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_raw.xlsx' \n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Separate the sample names, class labels, and features\n",
    "sample_names = data.iloc[:, 0]\n",
    "class_labels = data.iloc[:, 1]\n",
    "features = data.iloc[:, 2:]\n",
    "\n",
    "# Initialize the KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=best_k)\n",
    "\n",
    "# Perform KNN imputation on the entire dataset\n",
    "features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "# Convert the imputed features back to a DataFrame\n",
    "features_imputed_df = pd.DataFrame(features_imputed, columns=features.columns)\n",
    "\n",
    "# Combine the imputed features with sample names and class labels\n",
    "imputed_data = pd.concat([sample_names, class_labels, features_imputed_df], axis=1)\n",
    "\n",
    "# Save the imputed dataset to an Excel file\n",
    "output_file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_KNN.xlsx' \n",
    "output_file_path_2 = f'O2PLS_DA/{folder}/data/proteome/proteome_KNN_CV.xlsx' \n",
    "imputed_data.to_excel(output_file_path, index=False)\n",
    "imputed_data.to_excel(output_file_path_2, index=False)\n",
    "\n",
    "print(f\"KNN inputation finished with K = {best_k}\")\n",
    "\n",
    "\n",
    "\n",
    "################################################################ Outlier identification\n",
    "\n",
    "# Load the proteome dataset\n",
    "file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_KNN.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Separate the sample names, class labels, and features\n",
    "sample_names = data.iloc[:, 0]\n",
    "class_labels = data.iloc[:, 1]\n",
    "features = data.iloc[:, 2:]\n",
    "\n",
    "# Calculate the coefficient of variation (CV) for each protein\n",
    "cv_values = features.std(axis=0) / features.mean(axis=0) * 100\n",
    "\n",
    "# Filter out proteins with CV > 10%\n",
    "filtered_features = features.loc[:, cv_values <= 10]\n",
    "\n",
    "# Identify outliers\n",
    "outliers = cv_values[cv_values > 10]\n",
    "\n",
    "# Combine the filtered features with the sample names and class labels\n",
    "filtered_data = pd.concat([sample_names, class_labels, filtered_features], axis=1)\n",
    "\n",
    "# Save the outliers to a new Excel file\n",
    "outliers_df = pd.DataFrame({'Protein': outliers.index, 'CV%': outliers.values})\n",
    "outliers_df.to_excel(f'O2PLS_DA/{folder}/data/proteome/outliers_based_on_cv.xlsx', index=False)\n",
    "print(\"Outliers saved to 'outliers_based_on_cv.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1d5f2-2ee5-4f09-9c44-9a1dfab243ff",
   "metadata": {},
   "source": [
    "# Scaling and centering\n",
    "### Pareto scaling, unit variance scaling, mean centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62a4632-528a-42ac-b1dc-d38ec70aaeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pareto scaling on KNN_CV data done\n",
      "Unit variance scaling on KNN_CV data done\n",
      "Mean centering on KNN_CV data done\n",
      "script done\n"
     ]
    }
   ],
   "source": [
    "################################################# Pareto scaling \n",
    "\n",
    "# Load the log2 transformed data\n",
    "file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_KNN_CV.xlsx' \n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Separate sample names and classes from the features\n",
    "sample_names = data.iloc[:, 0]\n",
    "sample_classes = data.iloc[:, 1]\n",
    "features = data.iloc[:, 2:]\n",
    "\n",
    "# Ensure all data in features is numeric\n",
    "features = features.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Perform Pareto scaling\n",
    "mean = features.mean(axis=0)\n",
    "std = features.std(axis=0, ddof=1)  # Use sample standard deviation\n",
    "pareto_scaled_data = (features - mean) / np.sqrt(std)\n",
    "\n",
    "# Combine the sample names, classes, and scaled features into a new DataFrame\n",
    "pareto_scaled_df = pd.concat([sample_names, sample_classes, pareto_scaled_data], axis=1)\n",
    "\n",
    "# Save the scaled data to a new Excel file\n",
    "pareto_scaled_df.to_excel(f'O2PLS_DA/{folder}/data/proteome/proteome_KNN_CV_PS.xlsx', index=False, header=True)\n",
    "\n",
    "print(\"Pareto scaling on KNN_CV data done\")\n",
    "\n",
    "\n",
    "################################################# Unit variance scaling\n",
    "\n",
    "# Load the log2 transformed data\n",
    "file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_KNN_CV.xlsx' \n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Extract the sample names and classes\n",
    "sample_info = data.iloc[:, :2]\n",
    "\n",
    "# Extract the metabolite data\n",
    "metabolite_data = data.iloc[:, 2:]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(metabolite_data)\n",
    "\n",
    "# Create a new DataFrame with the scaled data\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=metabolite_data.columns)\n",
    "\n",
    "# Concatenate the sample info and the scaled data\n",
    "final_df = pd.concat([sample_info, scaled_df], axis=1)\n",
    "\n",
    "# Save the scaled data to a new Excel file\n",
    "scaled_file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_KNN_CV_UV.xlsx' \n",
    "final_df.to_excel(scaled_file_path, index=False)\n",
    "\n",
    "print(\"Unit variance scaling on KNN_CV data done\")\n",
    "\n",
    "\n",
    "################################################# Mean centering\n",
    "\n",
    "# Load the dataset\n",
    "file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_KNN_CV.xlsx' \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Set the sample name as the index\n",
    "df.set_index('Sample name', inplace=True)\n",
    "\n",
    "# Extract the sample class column and the data columns separately\n",
    "sample_class = df['Class']\n",
    "data = df.drop(columns=['Class'])\n",
    "\n",
    "# Calculate the mean for each metabolite (column-wise mean)\n",
    "means = data.mean()\n",
    "\n",
    "# Mean center the data by subtracting the mean of each metabolite\n",
    "mean_centered_data = data - means\n",
    "\n",
    "# Add the sample class column back to the mean centered data\n",
    "mean_centered_data.insert(0, 'Class', sample_class)\n",
    "\n",
    "# Display the first few rows of the mean centered data\n",
    "mean_centered_data.head()\n",
    "\n",
    "# Save the mean centered data to a new Excel file\n",
    "output_path = file_path = f'O2PLS_DA/{folder}/data/proteome/proteome_KNN_CV_centered.xlsx' \n",
    "mean_centered_data.to_excel(output_path, index=True)\n",
    "\n",
    "print(\"Mean centering on KNN_CV data done\")\n",
    "print(\"script done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
